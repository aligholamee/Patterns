\documentclass[12pt]{article}
\usepackage{latexsym,amssymb,amsmath} % for \Box, \mathbb, split, etc.
% \usepackage[]{showkeys} % shows label names
\usepackage{cite} % sorts citation numbers appropriately
\usepackage{path}
\usepackage{url}
\usepackage{verbatim}
\usepackage[pdftex]{graphicx}

% horizontal margins: 1.0 + 6.5 + 1.0 = 8.5
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
% vertical margins: 1.0 + 9.0 + 1.0 = 11.0
\setlength{\topmargin}{0.0in}
\setlength{\headheight}{12pt}
\setlength{\headsep}{13pt}
\setlength{\textheight}{625pt}
\setlength{\footskip}{24pt}

\renewcommand{\textfraction}{0.10}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\floatpagefraction}{0.90}

\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\makeatletter
\setlength{\arraycolsep}{2\p@} % make spaces around "=" in eqnarray smaller
\makeatother
\usepackage{stackengine}
% change equation, table, figure numbers to be counted inside a section:
\numberwithin{equation}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}

% begin of personal macros
\newcommand{\half}{{\textstyle \frac{1}{2}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\myth}{\vartheta}
\newcommand{\myphi}{\varphi}

\newcommand{\IN}{\mathbb{N}}
\newcommand{\IZ}{\mathbb{Z}}
\newcommand{\IQ}{\mathbb{Q}}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\IC}{\mathbb{C}}
\newcommand{\Real}[1]{\mathrm{Re}\left({#1}\right)}
\newcommand{\Imag}[1]{\mathrm{Im}\left({#1}\right)}

\newcommand{\norm}[2]{\|{#1}\|_{{}_{#2}}}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand{\ip}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\der}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\dder}[2]{\frac{\partial^2 {#1}}{\partial {#2}^2}}
\usepackage{enumitem}
\newcommand{\nn}{\mathbf{n}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\uu}{\mathbf{u}}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepackage{titlesec}
\newcommand{\junk}[1]{{}}
\usepackage{sectsty}
\usepackage{xcolor}

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
	\edef\arraystretch{#1}%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{*\c@MaxMatrixCols c}}
\makeatother

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{#1}}
\makeatother

\definecolor{darkblue}{rgb}{0,0,0.4}
\usepackage[colorlinks = true,
linkcolor = darkblue,
urlcolor  = darkblue,
citecolor = darkblue,
anchorcolor = darkblue]{hyperref}
% set two lengths for the includegraphics commands used to import the plots:
\newlength{\fwtwo} \setlength{\fwtwo}{0.45\textwidth}
% end of personal macros

\begin{document}
\DeclareGraphicsExtensions{.jpg}

\begin{center}
\textsc{\Large Statistical Pattern Recognition} \\[2pt]
	\textsc{\large Assignment 2}\\
	\vspace{0.5cm}
  Ali Gholami \\[6pt]
  Department of Computer Engineering \& Information Technology\\
  Amirkabir University of Technology  \\[6pt]
  \def\UrlFont{\em}
  \url{http://ceit.aut.ac.ir/~aligholamee}\\
    \href{mailto:aligholamee@aut.ac.ir}{\textit{aligholamee@aut.ac.ir}}
\end{center}

\begin{abstract}
In this assignment, we'll be focusing on the \textit{Bayes Classifier}. We'll work with \textit{Bayesian Discriminators} and \textit{Bayes Error}. The \textit{Bhattacharyya} error bound is also analyzed as an upper bound for the \textit{Bayes Classifier} error. The detailed computations of \textit{Bayesian Discriminators} are also given in an exact definition. Finally, we'll be going through a more practical example of a linear discriminator by classifying the flowers in the \textit{Iris} dataset.
\end{abstract}

\subparagraph{Keywords.} \textit{Linear Discriminator, Quadratic Discriminator, Bayes Classification, Bayes Error, Optimal Classification, Bhattacharyya Distance, Bhattacharyya Upper Bound, Iris Dataset, Iris Classification.}

\section{Quadratic \& Linear Discriminant Analysis}
We consider a classification problem in dimension $d=2$, with $k=3$ classes where:\\
$$
p(x\ |\ w_{i}) \sim N(\mu_{i}, \Sigma_{i}),\ \  i = 1, 2, 3
$$
and
$$
	\mu_1 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	0\\
	2\\
	\end{bmatrix},\ 
	\mu_2	 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	3\\
	1\\
	\end{bmatrix},\ 	
	\mu_3	 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	1\\
	0\\
	\end{bmatrix},
	\Sigma_{i}	 = \Sigma	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	1 & 0\\
	0 & \frac{1}{3}\\
	\end{bmatrix},
$$
\begin{enumerate}[label=(\alph*)]
	\item Calculate the discriminant function $g_{i}(x)$ for each class.
	
	\item Express your discriminant functions in the form of linear discriminant functions.
	
	\item Determine and plot the decision boundaries.
\end{enumerate}
\subsection*{Solution}
(a) The general form of a Bayesian discriminator is given below.
\begin{equation}\label{eq:1.1}
g_{i}(\underaccent{\bar}{x}) = 	-\frac{1}{2}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T}\Sigma_{i}^{-1}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i}) - \frac{1}{2}\log|\Sigma_{i}| + \log P(\omega_{i})
\end{equation}
In the problem case, the classes have the same covariance matrix, but the features have different variances.
Since the $\Sigma_{i}$ is diagonal, we'll have
$$
g_{i}(\underaccent{\bar}{x}) = 	-\frac{1}{2}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T}
	\begin{bmatrix}
\sigma_1^{-2} & 0 & 0 & \dots & 0\\
0 & \sigma_2^{-2} & 0 & \dots & 0\\
0 & 0 & \sigma_3^{-2} & \dots & 0\\ 
\vdots & \vdots & \vdots & \ddots & 0\\
0 & 0 & 0 & 0 & \sigma_N^{-2}\\
\end{bmatrix}
(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i}) - \frac{1}{2}\log \begin{vmatrix}
\sigma_1^{-2} & 0 & 0 & \dots & 0\\
0 & \sigma_2^{-2} & 0 & \dots & 0\\
0 & 0 & \sigma_3^{-2} & \dots & 0\\ 
\vdots & \vdots & \vdots & \ddots & 0\\
0 & 0 & 0 & 0 & \sigma_N^{-2}\\
\end{vmatrix} + \log P(\omega_{i})
$$
\end{document}