\documentclass[12pt]{article}
\usepackage{latexsym,amssymb,amsmath} % for \Box, \mathbb, split, etc.
% \usepackage[]{showkeys} % shows label names
\usepackage{cite} % sorts citation numbers appropriately
\usepackage{path}
\usepackage{url}
\usepackage{verbatim}
\usepackage[pdftex]{graphicx}

% horizontal margins: 1.0 + 6.5 + 1.0 = 8.5
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
% vertical margins: 1.0 + 9.0 + 1.0 = 11.0
\setlength{\topmargin}{0.0in}
\setlength{\headheight}{12pt}
\setlength{\headsep}{13pt}
\setlength{\textheight}{625pt}
\setlength{\footskip}{24pt}

\renewcommand{\textfraction}{0.10}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\floatpagefraction}{0.90}

\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\makeatletter
\setlength{\arraycolsep}{2\p@} % make spaces around "=" in eqnarray smaller
\makeatother
\usepackage{stackengine}
% change equation, table, figure numbers to be counted inside a section:
\numberwithin{equation}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}

% begin of personal macros
\newcommand{\half}{{\textstyle \frac{1}{2}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\myth}{\vartheta}
\newcommand{\myphi}{\varphi}

\newcommand{\IN}{\mathbb{N}}
\newcommand{\IZ}{\mathbb{Z}}
\newcommand{\IQ}{\mathbb{Q}}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\IC}{\mathbb{C}}
\newcommand{\Real}[1]{\mathrm{Re}\left({#1}\right)}
\newcommand{\Imag}[1]{\mathrm{Im}\left({#1}\right)}
\DeclareRobustCommand{\brkbinom}{\genfrac[]{0pt}{}}
\newcommand{\norm}[2]{\|{#1}\|_{{}_{#2}}}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand{\ip}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\der}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\dder}[2]{\frac{\partial^2 {#1}}{\partial {#2}^2}}
\usepackage{enumitem}
\newcommand{\nn}{\mathbf{n}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\uu}{\mathbf{u}}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepackage{titlesec}
\newcommand{\junk}[1]{{}}
\usepackage{sectsty}
\usepackage{xcolor}
\newcommand*{\bfrac}[2]{\genfrac{}{}{0pt}{}{#1}{#2}}
\newcommand\myatop[2]{\left[{{#1}\atop#2}\right]} % "wrapper macro"

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
	\edef\arraystretch{#1}%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{*\c@MaxMatrixCols c}}
\makeatother

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{#1}}
\makeatother

\definecolor{darkblue}{rgb}{0,0,0.4}
\usepackage[colorlinks = true,
linkcolor = darkblue,
urlcolor  = darkblue,
citecolor = darkblue,
anchorcolor = darkblue]{hyperref}
% set two lengths for the includegraphics commands used to import the plots:
\newlength{\fwtwo} \setlength{\fwtwo}{0.45\textwidth}
% end of personal macros

\begin{document}
\DeclareGraphicsExtensions{.jpg}

\begin{center}
\textsc{\Large Statistical Pattern Recognition} \\[2pt]
	\textsc{\large Assignment 2}\\
	\vspace{0.5cm}
  Ali Gholami \\[6pt]
  Department of Computer Engineering \& Information Technology\\
  Amirkabir University of Technology  \\[6pt]
  \def\UrlFont{\em}
  \url{http://ceit.aut.ac.ir/~aligholamee}\\
    \href{mailto:aligholamee@aut.ac.ir}{\textit{aligholamee@aut.ac.ir}}
\end{center}

\begin{abstract}
In this assignment, we'll be focusing on the \textit{Bayes Classifier}. We'll work with \textit{Bayesian Discriminators} and \textit{Bayes Error}. The \textit{Bhattacharyya} error bound is also analyzed as an upper bound for the \textit{Bayes Classifier} error. The detailed computations of \textit{Bayesian Discriminators} are also given in an exact definition. Finally, we'll be going through a more practical example of a linear discriminator by classifying the flowers in the \textit{Iris} dataset.
\end{abstract}

\subparagraph{Keywords.} \textit{Linear Discriminator, Quadratic Discriminator, Bayes Classification, Bayes Error, Optimal Classification, Bhattacharyya Distance, Bhattacharyya Upper Bound, Iris Dataset, Iris Classification.}

\section{Quadratic \& Linear Discriminant Analysis}
We consider a classification problem in dimension $d=2$, with $k=3$ classes where:\\
$$
p(x\ |\ w_{i}) \sim N(\mu_{i}, \Sigma_{i}),\ \  i = 1, 2, 3
$$
and
$$
	\mu_1 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	0\\
	2\\
	\end{bmatrix},\ 
	\mu_2	 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	3\\
	1\\
	\end{bmatrix},\ 	
	\mu_3	 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	1\\
	0\\
	\end{bmatrix},
	\Sigma_{i}	 = \Sigma =	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	1 & 0\\
	0 & \frac{1}{3}\\
	\end{bmatrix},
$$
\begin{enumerate}[label=(\alph*)]
	\item Calculate the discriminant function $g_{i}(x)$ for each class.
	
	\item Express your discriminant functions in the form of linear discriminant functions.
	
	\item Determine and plot the decision boundaries.
\end{enumerate}
\subsection*{Solution}
(a) The general form of a Bayesian discriminator is given below.
\begin{equation}\label{eq:1.1}
g_{i}(\underaccent{\bar}{x}) = 	-\frac{1}{2}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T}\Sigma_{i}^{-1}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i}) - \frac{1}{2}\log|\Sigma_{i}| + \log P(\omega_{i})
\end{equation}
In the problem case, the classes have the same covariance matrix, but the features have different variances.
Since the $\Sigma_{i}$ is diagonal, we'll have
$$
g_{i}(\underaccent{\bar}{x}) = 	-\frac{1}{2}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T}
	\begin{bmatrix}
\sigma_1^{-2} & 0 & 0 & \dots & 0\\
0 & \sigma_2^{-2} & 0 & \dots & 0\\
0 & 0 & \sigma_3^{-2} & \dots & 0\\ 
\vdots & \vdots & \vdots & \ddots & 0\\
0 & 0 & 0 & 0 & \sigma_N^{-2}\\
\end{bmatrix}
(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i}) - \frac{1}{2}\log \begin{vmatrix}
\sigma_1^{-2} & 0 & 0 & \dots & 0\\
0 & \sigma_2^{-2} & 0 & \dots & 0\\
0 & 0 & \sigma_3^{-2} & \dots & 0\\ 
\vdots & \vdots & \vdots & \ddots & 0\\
0 & 0 & 0 & 0 & \sigma_N^{-2}\\
\end{vmatrix} + \log(P(\omega_{i}))
$$
Since we have the following criteria:
$$
	(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T} = \begin{bmatrix}
	x[1] - \mu_{i}[1]\\
	x[2] - \mu_{i}[2]\\
	x[3] - \mu_{i}[3]\\
	x[4] - \mu_{i}[4]\\
	\vdots\\
	x[N] - \mu_{i}[N]\\
	\end{bmatrix}
$$
where $\mu_{iN}$ denotes the \textit{N}'th feature of class \textit{i}. Removing the constant term for different classes, which is $ x[k]^2$, we'll have the following results after the matrix multiplication and determinant computation:
\begin{equation}
	g_{i}(\underaccent{\bar}{x}) = -\frac{1}{2}\sum_{k=1}^{N}\frac{2x[k]\mu_{i}[k] + \mu_{i}[k]^2}{\sigma_k^2} - \frac{1}{2}\log\prod_{k=1}^{N}\sigma_k^2 + \log(P(\omega_i))
\end{equation}

One can simply find each discriminator, $g_{i}(\underaccent{\bar}{x})$, by replacing the given information in the problem description in the formula given above. Thus we'll have the following results for the section (a).
$$
	g_{1}(\underaccent{\bar}{x}) = -\frac{1}{2}(\frac{2x[1]*0 + 2}{1} + \frac{2x[2]*2 + 4}{\frac{1}{9}}) - \frac{1}{2}\log(1*\frac{1}{9}) + ?
$$
$$
g_{2}(\underaccent{\bar}{x}) = -\frac{1}{2}(\frac{2x[1]*3 + 3}{1} + \frac{2x[2]*1 + 1}{\frac{1}{9}}) - \frac{1}{2}\log(1*\frac{1}{9}) + ?
$$
$$
g_{3}(\underaccent{\bar}{x}) = -\frac{1}{2}(\frac{2x[1]*1 + 1}{1} + \frac{2x[2]*0 + 0}{\frac{1}{9}}) - \frac{1}{2}\log(1*\frac{1}{9}) + ?
$$
The simplified results are
$$
	g_{1}(\underaccent{\bar}{x}) = -18x[2] - \frac{1}{2}\log\frac{1}{9} - 19
$$
$$
	g_{2}(\underaccent{\bar}{x}) = -3x[1] + 9x[2] - \frac{1}{2}\log\frac{1}{9} - 6
$$
$$
	g_{3}(\underaccent{\bar}{x}) = -x[1] - \frac{1}{2}\log\frac{1}{9} - \frac{1}{2}
$$
\noindent\rule{\textwidth}{.5pt}
(b) The final results given above where in the format of a linear discriminant already. In order to lighten everything up, just assume the linear discriminant function as:
$$
	g_{i}(\underaccent{\bar}{x}) = W_{2}x[2] + W_{1}x[1] + W_0
$$
where the value of $W_i$ is different for each of the discriminators.
$$
	g_{1}(\underaccent{\bar}{x}) \ \ \ W_2 = -18 \ \ W_1 = 0 \ \ W_0 =  - \frac{1}{2}\log\frac{1}{9} - 19
$$
$$
g_{2}(\underaccent{\bar}{x}) \ \ \ W_2 = 9 \ \ W_1 = -3 \ \ W_0 =  - \frac{1}{2}\log\frac{1}{9} - 6
$$
$$
g_{1}(\underaccent{\bar}{x}) \ \ \ W_2 = 0 \ \ W_1 = -1 \ \ W_0 =  - \frac{1}{2}\log\frac{1}{9} - \frac{1}{2}
$$
Each of the $g_{i}(\underaccent{\bar}{x})$ represent a discriminator plane in the \textit{3D} space.

\noindent\rule{\textwidth}{.5pt}
(c) Here are the plots of distributions and discriminators below. These are coded in Python using \textit{PyLab}.
\begin{figure}[!h]\centering
	\includegraphics[width=0.6\textwidth]{1_c_1.png}
	\caption{Distributions of three classes described in the problem description.}
	\label{pl1}
\end{figure}
\begin{figure}[!h]\centering
	\includegraphics[width=0.6\textwidth]{1_c_2.png}
	\caption{Linear discriminators of Figure 1.1 distributions.}
	\label{pl2}
\end{figure}

\section{Bayes Decision Rule \& Bayes Error Boundaries}
Consider the following 2-class classification problem involving a single feature $x$. Assume equal class priors and $0-1$ loss function.
$$
p(x\ |\ w_1) = \begin{cases} 
2x &  0 \leq x \leq 1\\
0       &  otherwise
\end{cases} \ \ \ \ p(x\ |\ w_2) = \begin{cases} 
2-2x &  0 \leq x \leq 1\\
0       &  otherwise
\end{cases}
$$	
\begin{enumerate}[label=(\alph*)]
	\item Sketch the two densities.
	
	\item State the Bayes decision rule and show the decision boundary.
	
	\item What is the Bayes classification error?
	
	\item How will the decision boundary change if the prior for class w1 is increased to 0.7? 
\end{enumerate}
\section*{Solution}
(a) Figure 2.1, illustrates the density functions of these two classes. I've used the \textit{Seaborn} library to generate these density functions.
\begin{figure}[!h]\centering
	\includegraphics[width=0.6\textwidth]{2_1_1.png}
	\caption{Illustration of density functions of $w_1$(Blue) and $w_2$(Orange).}
	\label{density}
\end{figure}

\noindent\rule{\textwidth}{.5pt}
(b) We drive the Bayes decision rule for these two classes below. $g_1(x)$ and $g_2(x)$ represent the decision function for the classes 1 and 2 respectively.

$$
	g_i(x)\ \bfrac{\overset{\omega_i}{>}}{\overset{\omega_j}{<}}\ g_j(x)
$$
which is our decision baseline for the Bayes classifier. Since $g_i(x) = p(\omega_1\ |\ x)$, expanding the equation according to the Bayes rule and we get:
$$
	g_i(x) = \frac{p(\underaccent{\bar}{x}\ |\ \omega_i)P(\omega_i)}{p(\underaccent{\bar}{x})}
$$
Replacing the $g_i(x)$ in the decision baseline and we'll have the following results.
$$
	\frac{p(\underaccent{\bar}{x}\ |\ \omega_i)P(\omega_i)}{p(\underaccent{\bar}{x})}\  \bfrac{\overset{\omega_i}{>}}{\overset{\omega_j}{<}}\ \frac{p(\underaccent{\bar}{x}\ |\ \omega_j)P(\omega_j)}{p(\underaccent{\bar}{x})}
$$
Omitting the constant parts from both sides and replacing the equations from the problem description will result in the following decision function.
\begin{equation}
	g(x) = 4x - 2\ \bfrac{\overset{\omega_i}{>}}{\overset{\omega_j}{<}}\ 0
\end{equation}
Thus, the linear discriminant function can be displayed as so:
$$
	g(x) = 4x - 2
$$
in which the point $x = \frac{1}{2}$ is the separation point of two classes. The values greater than $\frac{1}{2}$ are assigned a label from class $i$. The values less the $\frac{1}{2}$ are assigned a label of class $j$.

\noindent\rule{\textwidth}{.5pt}
(c) Here is the Bayes classification error given in (2.2).
\begin{equation}
	\eps = \eps_1P(\omega_1) + \eps_2P(\omega_2)
\end{equation}
in which the $\eps_1$ and $\eps_2$ represent the probability of class 1 error by integrating the class 1 density over the region of class 2 and the probability of class 2 error by integrating the class 2 density over the region of class 1 respectively.
$$
	\eps_1 = \int_{R_2} p(\underaccent{\bar}{x}\ |\ \omega_1)d\underaccent{\bar}{x}
$$
$$
	\eps_2 = \int_{R_1} p(\underaccent{\bar}{x}\ |\ \omega_2)d\underaccent{\bar}{x}
$$
According the section (b), the discriminating point is $x = 0.5$. Correspondingly, the regions $R_1$ and $R_2$ can be easily driven like so:
$$
	R_1 = [0\ 0.5] \ \ \ \ R_2 = [0.5\ 1]
$$
By integrating the given equation (2.2) over the boundaries of these two regions, we'll have the following:
$$
	\eps_1 = \int_{0}^{0.5}(2x) dx = \frac{1}{4}
$$
$$
	\eps_2 = \int_{0.5}^{1} (2-2x)dx = \frac{1}{4}
$$
The final value for the Bayes error will be:
$$
	\eps = \frac{1}{4} * \frac{1}{2} + \frac{1}{4} * \frac{1}{2} = \frac{1}{4}
$$

\noindent\rule{\textwidth}{.5pt}
(d) Changing the prior probabilities for classes $\omega_1$ and $\omega_2$, the bias will be changed. We'll have the following biases as the prior probabilities.
$$
	P(\omega_1) = 0.7
$$
$$
	P(\omega_2) = 0.3
$$
Rewriting the likelihood ratio for these two classes, we'll have the following results:
$$
	\frac{p(\underaccent{\bar}{x}\ |\ \omega_1)}{p(\underaccent{\bar}{x}\ |\ \omega_2)}\  \bfrac{\overset{\omega_1}{>}}{\overset{\omega_2}{<}}\ \frac{P(\omega_2)}{P(\omega_1)}
$$
$$
	\frac{2x}{2-2x}\  \bfrac{\overset{\omega_1}{>}}{\overset{\omega_2}{<}}\ \frac{3}{7}
$$
which changes the final discriminant function, $g(x)$ to
$$
	g'(x) = 10x - 3\ \bfrac{\overset{\omega_1}{>}}{\overset{\omega_2}{<}}\ 0
$$

\section{Bayes Decision Boundary \& Bhattacharyya Error Bound}
Consider a two-category classification problem in two dimensions with
$$
p(x\ |\ w_{1}) \sim N(0, I),\ \ \ p(x\ |\ w_{2}) \sim N(\begin{bmatrix}
1\\
1\\
\end{bmatrix}, I)
$$
and
$$
P(\omega_1) = P(\omega_2) = \frac{1}{2}
$$
\begin{enumerate}[label=(\alph*)]
	\item Calculate the Bayes Decision Boundary.
	
	\item Calculate the Bhattacharyya error bound.
	
	\item Repeat the above for the same probabilities, but
	$$
	p(x\ |\ w_{1}) \sim N(0, \begin{bmatrix}
	2 & 0.5\\
	0.5 & 2\\
	\end{bmatrix}),\ \ \ p(x\ |\ w_{2}) \sim N(\begin{bmatrix}
	1\\
	1\\
	\end{bmatrix}, \begin{bmatrix}
	5 & 4\\
	4 & 5\\
	\end{bmatrix})
	$$
\end{enumerate}
\subsection*{Solution}
(a) The genera for of a Bayesian discriminator was discussed in (1.1). Comparing this to the linear classifier which can be displayed as below:
$$
	g_i(\underaccent{\bar}{x}) = W_{i1}^T\underaccent{\bar}{x} + W_{i0}
$$
where the weights are also given below:\\
$$
	W_{i1} = \frac{\mu_i}{\sigma^2}
$$
$$
	W_{i0} = \frac{-1}{2\sigma^2}\mu_{i}^T\mu_{i} + \log P(\omega_i)
$$
Since $\Sigma_i = \sigma^2I$ with $\sigma^2 = 1$(according to problem description) with equal prior probabilities($P(\omega_1) = P(\omega_2) = \frac{1}{2}$, the discriminator can be derived using the \textit{Euclidean Distance} of $x$ and $\mu$.
$$
	g_i(\underaccent{\bar}{x}) = (\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_i)^T (\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_i)
$$
we can simply find the $g_1(\underaccent{\bar}{x})$ and $g_2(\underaccent{\bar}{x})$ by replacing the parameters in the problem description. We'll get:
$$
	g_1(\underaccent{\bar}{x}) = (\underaccent{\bar}{x} - 0)^T(\underaccent{\bar}{x} - 0) = (\underaccent{\bar}{x})^2
$$	
$$
	g_2(\underaccent{\bar}{x}) = (\underaccent{\bar}{x} - \begin{bmatrix}
	1\\
	1\\
	\end{bmatrix})^T(\underaccent{\bar}{x} - \begin{bmatrix}
	1\\
	1\\
	\end{bmatrix}) = (\underaccent{\bar}{x} - 1)^2
$$	
In order to find the decision boundary, we'll coincide the two discriminators:
$$
	\underaccent{\bar}{x} = \frac{1}{2}
$$
Meaning that the decision boundary is the plane $\underaccent{\bar}{x} = \frac{1}{2}$.

\noindent\rule{\textwidth}{.5pt}
(b) The \textit{Bhattacharyya} error bound is an specific condition of \textit{Chernoff} error bound. This condition happens when $s = \frac{1}{2}$ in the \textit{Chernoff} bound formula. The \textit{Bhattacharyya} formula is given below.
\begin{equation}
	\eps_{n-B} = \sqrt{P(\omega_1) P(\omega_2)} \int \sqrt{p(\underaccent{\bar}{x}\ |\ \omega_1)p(\underaccent{\bar}{x}\ |\ \omega_2)}d\underaccent{\bar}{x} = e^{-\mu(s = \frac{1}{2})}
\end{equation}
If we have access to the parameters of two distributions we can derive the \textit{Bhattacharyya} error bound by computing the $\mu(s = \frac{1}{2})$ and replacing the result in the $e^{-\mu(s = \frac{1}{2})}$.
$$
	\mu(\frac{1}{2}) = \frac{1}{8}(\underaccent{\bar}{m}_2 - \underaccent{\bar}{m}_1)^T(\frac{\Sigma_1 + \Sigma_2}{2})^{-1}(\underaccent{\bar}{m}_2 - \underaccent{\bar}{m}_1) + \frac{1}{2}\ln \frac{|\frac{\Sigma_1 + \Sigma_2}{2}|}{\sqrt{|\Sigma_1| |\Sigma_2|}}
$$
after some minor matrix multiplication, we'll get the following results:
$$
	\mu(\frac{1}{2}) = (\frac{1}{8})(4) = \frac{1}{2}
$$
The \textit{Bhattacharyya} error bound will be $\eps_{n-B} = e^{-\frac{1}{2}}$.

\noindent\rule{\textwidth}{.5pt}
(c) $g_1(\underaccent{\bar}{x})$ and $g_2(\underaccent{\bar}{x})$ can be easily computed using the \textit{general form} of the Bayes classifier.
$$
	g_1(\underaccent{\bar}{x}) = -\frac{1}{2}(\underaccent{\bar}{x} - 0)^T\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}\begin{bmatrix}
	\frac{8}{15} & \frac{2}{15}\\
	\frac{2}{15} & \frac{8}{15}\\
	\end{bmatrix}(\underaccent{\bar}{x} - 0) - \frac{1}{2}\log \frac{15}{4} + \log \frac{1}{2}
$$
$$
	g_2(\underaccent{\bar}{x}) = -\frac{1}{2}(\underaccent{\bar}{x} - \begin{bmatrix}
	1\\
	1\\
	\end{bmatrix})^T\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}\begin{bmatrix}
	\frac{5}{9} & \frac{4}{9}\\
	\frac{4}{9} & \frac{5}{9}\\
	\end{bmatrix}(\underaccent{\bar}{x} - \begin{bmatrix}
	1\\
	1\\
	\end{bmatrix}) - \frac{1}{2}\log 9 + \log \frac{1}{2}
$$
Coinciding the two decision functions and we get the following decision boundary:
$$
	\underaccent{\bar}{x}^T\Sigma_{1}^{-1}\underaccent{\bar}{x} - (\underaccent{\bar}{x} -1)^T\Sigma_{2}^{-1}(\underaccent{\bar}{x} -1) + \log \frac{5}{12} = 0
$$
which appears to be a \textit{Hyper-ellipsoid}. Furthermore, the \textit{Bhattacharyya} error bound can be calculated as following.
$$
	\mu(\frac{1}{2}) = \frac{1}{8}(\begin{bmatrix}
	1\\
	1\\
	\end{bmatrix} - \begin{bmatrix}
	0\\
	0\\
	\end{bmatrix})^T(\frac{\begin{bmatrix}
		7 & 4.5\\
		4.5 & 7\\
		\end{bmatrix}}{2})^{-1}(\begin{bmatrix}
	1\\
1\\
\end{bmatrix} - \begin{bmatrix}
0\\
0\\
\end{bmatrix}) + \frac{1}{2}\ln \frac{\frac{\begin{vmatrix}
		7 & 4.5\\
		4.5 & 7\\
		\end{vmatrix}}{2}}{\sqrt{\begin{vmatrix}
		2 & 0.5\\
		0.5 & 2\\
		\end{vmatrix}\begin{vmatrix}
		5 & 4\\
		4 & 5\\
		\end{vmatrix}}}
$$
which results in $\mu(\frac{1}{2}) = -0.48$. Finally, the \textit{Bhattacharyya} error bound result is given below.
$$
	\eps_{n-B} = e^{0.48}
$$

\section{Bayes Decision Boundary \& Dataset Samples}
Consider the two-dimensional data points from two classes $\omega_1$ and $\omega_2$ below. Each of them are coming from a Gaussian distribution $p(x\ |\ \omega_k) \sim N(\mu_k, \Sigma_k)$.

\begin{table}[!h] \centering
	\begin{tabular}{rcc}
		\hline
		$ \omega_1 $\ \ \ &
		$ \omega_2 $ &\\
		\hline
		(0, 0) & (6, 9)\\
		(0, 1) & (8, 9)\\
		(2, 2) & (9, 8)\\
		(3, 1) & (9, 9)\\
		(3, 2) & (9, 10)\\
		(3, 3) & (8, 11)\\
		\hline
	\end{tabular}
	\caption{Data points from class $\omega_1$ and $\omega_2$.}
	\label{tabconvdemo}
\end{table}

\begin{enumerate}[label=(\alph*)]
	\item What is the prior probability for each class? 
	
	\item Calculate the mean and covariance matrix for each class. 
	
	\item Derive the equation for the decision boundary that separates these two classes, and plot the boundary.
	
	\item Think of the case that the penalties for misclassification are different for the two classes (i.e. not zero-one loss), will it affect the decision boundary, and how?
\end{enumerate}

\subsection*{Solution}
(a) The prior probability can be estimated using the following formula.
\begin{equation}
	P(\omega_i) = \frac{|S|\ \ \epsilon\ \omega_i}{|S|}
\end{equation}
this value is the same for both of these classes and its equal to $P(\omega_1) = P(\omega_2) = \frac{1}{2}$.

\noindent\rule{\textwidth}{.5pt}
(b) For the covariance matrix, diagonal elements are computed as below.
$$
	\sigma_{11}^2 = \frac{\sum_{i=1}^{N}x_1[i]}{N} - \mu_{11} \ \ \ 
	\sigma_{22}^2 = \frac{\sum_{i=1}^{N}x_2[i]}{N} - \mu_{12} 
$$
the non-diagonal elements are computed as below.
$$
	\sigma_{12}^2 = \frac{1}{n}\sum_{i=1}^{N}(x_i - E[x])(y_i - E[x])
$$
and same for the other element. The results would be:
$$
	\underaccent{\bar}{\mu}_1 = \begin{bmatrix}
	1.83\\
	1.5\\
	\end{bmatrix} \ \ 
	\underaccent{\bar}{\mu}_2 = \begin{bmatrix}
	8.16\\
	9.33\\
	\end{bmatrix} \ \ 
	\Sigma_1 = \begin{bmatrix}
	2.16 & 1.1\\
	1.1 & 1.1\\
	\end{bmatrix} \ \
	\Sigma_2 = \begin{bmatrix}
	1.36 & -0.06\\
	-0.06 & 1.06\\
	\end{bmatrix}
$$

\noindent\rule{\textwidth}{.5pt}
(c) The general case for the Bayes decision is given below. formerly, we have been working with the exact equation given in (1.2).

\begin{equation}
	g_i(\underaccent{\bar}{x}) = \underaccent{\bar}{x}^TW_i\underaccent{\bar}{x} + w_i^T\underaccent{\bar}{x} + w_{i0}
\end{equation}
The weights are computed as following.
$$
	W_i = -\frac{1}{2}\Sigma_i^{-1}
$$
$$
	w_i = \Sigma_i^{-1}\underaccent{\bar}{\mu}_i
$$
$$
	w_{i0} = -\frac{1}{2}\underaccent{\bar}{\mu}_i^T\Sigma_i^{-1}\underaccent{\bar}{\mu}_i - \frac{1}{2}\log(|\Sigma_i|) + \log P(\omega_i)
$$
Saving some time, we'll be devoting the burden of this computation to Python!. The result will be:
$$
	W_1 =  \begin{bmatrix}
	-0.46 & 0.46\\
	0.46 & -0.92\\
	\end{bmatrix}
$$
$$
	w_1 = \begin{bmatrix}
	0.31 & 1.05\\
	\end{bmatrix}
$$
$$
	w_{10} = -1.84
$$
The corresponding result will be as following for the second discriminator.
$$
	W_1 =  \begin{bmatrix}
	-0.36 & -0.02\\
	-0.02 & -0.47\\
	\end{bmatrix}
$$
$$
	w_1 = \begin{bmatrix}
	6.42 & 9.15\\
	\end{bmatrix}
$$
$$
	w_{10} = -69.80
$$
Simplifying the weights given above in the equation $g_1(x) = g_2(x)$, we'll get the following results.
\begin{equation}
	-0.1x^2 - 0.45y^2 - 6.11x - 8.1y + 0.96xy + 67.96= 0
\end{equation}
Here is the plotted results.
\begin{figure}[!h]\centering
	\includegraphics[width=0.6\textwidth]{4_3.png}
	\caption{Contour lines for the Bayesian discriminator (4.3)}
	\label{pl1}
\end{figure}
	
\noindent\rule{\textwidth}{.5pt}
(d) If the classification is based on the misclassification penalty; for example when the penalty for the class 1 being misclassified as class 2 is 0.08 and the other penalty is 0.02, the classifier tends to assign most of the samples to the class 2 because it will cost less. So, changing the misclassification penalty would definitely exerts influence on the classification task.

\section{Decision Boundaries of Exponential \& Uniform Distributions}
Consider a classification problem with 2 classes and a single real-valued feature vector $X$. for class 1, $p(x\ |\ c_1)$ is uniform $U(a, b)$ with $a=2$ and $b=4$. For class 2, $p(x\ |\ c_2)$ is exponential with density $\lambda \exp(-\lambda x)$ where $\lambda=1$. Let $P(c_1)=P(c_2)=0.5$.

\begin{enumerate}[label=(\alph*)]
	\item Determine the location of optimal decision regions.
	
	\item Draw a sketch of the two class densities multiplied by $P(c_1)$ and $P(c_2)$ respectively, as a function of $x$, clearly showing the optimal decision boundary.
	
	\item Compute the Bayes error rate for this problem within 3 decimal places of accuracy.
	
	\item Answer the questions above with $a=2$ and $b=22$.
\end{enumerate}
\subsection*{Solution}
We'll derive the \textit{log likelihood} for these two classes.
$$
	\frac{p(\underaccent{\bar}{x}\ |\ c_1)}{p(\underaccent{\bar}{x}\ |\ c_2)}\  \bfrac{\overset{c_1}{>}}{\overset{c_2}{<}}\ \frac{P(c_2)}{P(c_1)}
$$
The density function for the uniform distribution on an interval $[a, b]$ is equal to $f(x) = \frac{1}{b-a}$. In this case we'll have $p(\underaccent{\bar}{x}\ |\ c_1) = \frac{1}{2}$. Replacing the densities in the \textit{likelihood ratio}:
$$
	\frac{\frac{1}{2}}{\exp(-x)}\  		\bfrac{\overset{c_1}{>}}{\overset{c_2}{<}}\ \frac{\frac{1}{2}}{\frac{1}{2}}
$$	
Thus, the \textit{log likelihood} will be:
$$
	\ln \frac{1}{2} - \ln \exp(-x) \		\bfrac{\overset{c_1}{>}}{\overset{c_2}{<}}\ 0
$$	
$$
	d(x) = x - 0.693\ 		\bfrac{\overset{c_1}{>}}{\overset{c_2}{<}}\ 0
$$	
in which $d(x)$ is the decision boundary for the given classes.
\newpage

(b) Here is the result for after plotting these two distributions. The green line illustrates the decision boundary.
\begin{figure}[!h]\centering
	\includegraphics[width=0.6\textwidth]{5_2.png}
	\caption{Class 1 and Class 2 Distribution Plots.}
	\label{pl1}
\end{figure}

\noindent\rule{\textwidth}{.5pt}
(c) According to the figure (5.2), the Bayes error can be calculated by integrating the exponential density over the period [2, 4].
\begin{figure}[!h]\centering
	\includegraphics[width=0.6\textwidth]{5_3.png}
	\caption{Bayes Error Surface Plot.}
	\label{pl1}
\end{figure}
$$
	\eps = \int_{2}^{4}\frac{1}{2}\exp(-x) = -0.058
$$

\noindent\rule{\textwidth}{.5pt}
(d) We repeat the whole process with $a=2$ and $b=22$ again. The decision boundary will be as following:
\begin{figure}[!h]\centering
	\includegraphics[width=0.6\textwidth]{5_4.png}
	\caption{Bayes Error Surface Plot.}
	\label{pl1}
\end{figure}
$$
	d(x) = x - 2.995\  \bfrac{\overset{c_1}{>}}{\overset{c_2}{<}}\
$$
Here is the results after plotting the decision boundary(green line).
The Bayes error for this condition is given below. The interval is $2 < x < 22$:
$$
	\eps = \int_{2}^{22}\frac{1}{2}\exp(-x) = -0.067
$$

\section{Linear Discriminant Analysis \& Bayes Classifier}
Consider a 2-class classification problem with d-dimensional real-valued inputs x, where the class-conditional densities, $p(x\ |\ c_1)$ and $p(x\ |\ c_2)$ are multivariate Gaussian with different means $\underaccent{\bar}{\mu}_1$ and $\underaccent{\bar}{\mu}_2$ and a common covariance matrix $\Sigma$, with class probabilities $P(c_1)$ and $P(c_2)$.
\begin{enumerate}[label=(\alph*)]
	\item Write the discriminant function for this problem in the form of $g_1(\underaccent{\bar}{x}) = \log p(\underaccent{\bar}{x}\ |\ c_1) + \log P(c_1).$
	
	\item prove that the optimal decision boundary can be written in the form of a linear discriminant, $\underaccent{\bar}{w}\underaccent{\bar}{x} + w_0 = 0$, where $\underaccent{\bar}{w}$ is a d-dimensional weight vector and $w_0$ is a scalar, and clearly indicate what are $\underaccent{\bar}{w}$ and $w_0$ are in terms of parameters of the classification model.
\end{enumerate}
\subsection*{Solution}
The multivariate normal density function is defined as
\begin{equation}\label{eq:6.1}
	p(\underaccent{\bar}{x}\ |\ \mu, \Sigma) = \frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}} e^{\frac{-1}{2}(\underaccent{\bar}{x} - \mu)^T\Sigma^{-1}(\underaccent{\bar}{x} - \mu)} 
\end{equation}
Using Bayes rule, the \textit{MAP} discriminant function becomes
$$
	g_1(\underaccent{\bar}{x}) = p(\omega_i\ |\ \underaccent{\bar}{x}) = \frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}} e^{\frac{-1}{2}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_1)^T\Sigma^{-1}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_1)}P(\omega_i)\frac{1}{p(\underaccent{\bar}{x})}
$$
Eliminating the constant term($p(\underaccent{\bar}{x})$ which is not considered in discrimination) and taking natural log(since the logarithm is monotonically increasing function)
$$
	g_1(\underaccent{\bar}{x}) = -\frac{1}{2}(\underaccent{\bar}{x}-\underaccent{\bar}{\mu}_1)\Sigma^{-1}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_1) - \frac{1}{2}\log(|\Sigma|) + \log(P(\omega_1))
$$
Almost the same condition happens for the $g_2(\underaccent{\bar}{x})$ with minor index changes:
$$
	g_2(\underaccent{\bar}{x}) = -\frac{1}{2}(\underaccent{\bar}{x}-\underaccent{\bar}{\mu}_2)\Sigma^{-1}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_2) - \frac{1}{2}\log(|\Sigma|) + \log(P(\omega_1))
$$
\noindent\rule{\textwidth}{.5pt}
(b) This can be gone further by removing the constant term of $\log(|\Sigma|)$ from the above discriminants. We'll can derive the following results.
$$
		g_i(\underaccent{\bar}{x}) = -\frac{1}{2}(\underaccent{\bar}{x}-\underaccent{\bar}{\mu}_2)\Sigma^{-1}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_2) + \log(P(\omega_1))
$$
Expanding the quadratic term yields
$$
		g_i(\underaccent{\bar}{x}) = -\frac{1}{2}(\underaccent{\bar}{x}^T\Sigma^{-1} \underaccent{\bar}{x} - 2\mu_i^T\Sigma^{-1}\underaccent{\bar}{x}  + \underaccent{\bar}{\mu}_i^{T}\Sigma^{-1}\underaccent{\bar}{\mu}_i) + \log(P(\omega_1))
$$
Removing the term $\underaccent{\bar}{x}^T\Sigma^{-1}\underaccent{\bar}{x}$ which is constant for all classes. Reorganizing terms we obtain 
\begin{equation}
	g_i(\underaccent{\bar}{x}) = w_i^Tx + w_{i0}
\end{equation}
where $w_i$ and $w_{i0}$ are defined as:
$$
	w_i(\underaccent{\bar}{x}) = \Sigma^{-1}\underaccent{\bar}{\mu}_i \ \ \ \ \ \ 
	w_{i0} = -\frac{1}{2}\underaccent{\bar}{\mu}_i^{T}\Sigma^{-1}\underaccent{\bar}{\mu}_i + \log P(\omega_i)
$$

\newpage
\section{Bayesian Decision Plot}
\subsection*{Solution}
Here is the result after plotting the both points and Bayesian discriminator. Please refer to the \textit{plot.py} in the directory of problem 7.
\begin{figure}[!h]\centering
	\includegraphics[width=0.5\textwidth]{7.png}
	\caption{Decision region for the given scattered points.}
	\label{pl7}
\end{figure}

\section{Bayesian Classification of IRIS Dataset}
\subsection*{Solution}
(a) Firstly, we'll provide the histogram plots for the 2 given classes.
\begin{figure}[!h]\centering
	\includegraphics[width=0.8\textwidth]{8_1.png}
	\caption{Histogram of Iris Setosa and Iris Veriscolor.}
	\label{pl8}
\end{figure}\\
(b) Estimating the class priors from the training data. Here is the results.
$$
	P(Setosa) = 0.113 \ \ \ \ P(Veriscolor) = 0.086
$$ 
these numbers are also provided in the code for this section. Please refer to problem 8 sources for more details.\\
(c) Mean and variance of both classes is given below.
$$
	\mu_{Setosa}:  3.399999999999999
$$
$$
	\sigma_{Setosa}:  0.1328205128205128
$$
$$
	\mu_{Veriscolor}:  2.783333333333333
$$
$$
	\sigma_{Veriscolor}:  0.11072222222222221
$$\\
(d) Here is the density plot for both classes.
\begin{figure}[!h]\centering
	\includegraphics[width=0.8\textwidth]{8_3.png}
	\caption{Density Plot of Iris Setosa and Iris Veriscolor.}
	\label{pl8_3}
\end{figure}
\newpage
We can derive the Bayes decision boundary by writing down the \textit{log likelihood} ratio. The result is given in the code(section 8) and the plot is given here. Class posteriors can be computed by expanding the Bayes formula and replacing the mean and variance of each class.
$$
	p(c_i\ |\ x) = \frac{p(x\ |\ c_i)P(c_i)}{p(x)} = \frac{1}{\sqrt{2\pi}\sigma_i}\exp(-\frac{(x - \mu_i)^2}{2\sigma_i^2})P(c_i)
$$
After removing the constant term $p(x)$ and performing a log likelihood, we'll get:
$$
	d(x) = \ln\frac{\sigma_2}{\sigma_1} - \frac{(x-\mu_1)^2}{2\sigma_1^2} + \frac{(x-\mu_2)^2}{2\sigma_2^2} - \ln\frac{P(c_2)}{c_2} = 0
$$
\begin{figure}[!h]\centering
	\includegraphics[width=0.8\textwidth]{8_4.png}
	\caption{Decision Boundary of Iris Setosa and Iris Veriscolor.}
	\label{pl8_4}
\end{figure}\\

(e) 
\end{document} 
