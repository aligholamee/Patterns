\documentclass[12pt]{article}
\usepackage{latexsym,amssymb,amsmath} % for \Box, \mathbb, split, etc.
% \usepackage[]{showkeys} % shows label names
\usepackage{cite} % sorts citation numbers appropriately
\usepackage{path}
\usepackage{url}
\usepackage{verbatim}
\usepackage[pdftex]{graphicx}

% horizontal margins: 1.0 + 6.5 + 1.0 = 8.5
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
% vertical margins: 1.0 + 9.0 + 1.0 = 11.0
\setlength{\topmargin}{0.0in}
\setlength{\headheight}{12pt}
\setlength{\headsep}{13pt}
\setlength{\textheight}{625pt}
\setlength{\footskip}{24pt}

\renewcommand{\textfraction}{0.10}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\floatpagefraction}{0.90}

\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\makeatletter
\setlength{\arraycolsep}{2\p@} % make spaces around "=" in eqnarray smaller
\makeatother
\usepackage{stackengine}
% change equation, table, figure numbers to be counted inside a section:
\numberwithin{equation}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}

% begin of personal macros
\newcommand{\half}{{\textstyle \frac{1}{2}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\myth}{\vartheta}
\newcommand{\myphi}{\varphi}

\newcommand{\IN}{\mathbb{N}}
\newcommand{\IZ}{\mathbb{Z}}
\newcommand{\IQ}{\mathbb{Q}}
\newcommand{\IR}{\mathbb{R}}
\newcommand{\IC}{\mathbb{C}}
\newcommand{\Real}[1]{\mathrm{Re}\left({#1}\right)}
\newcommand{\Imag}[1]{\mathrm{Im}\left({#1}\right)}

\newcommand{\norm}[2]{\|{#1}\|_{{}_{#2}}}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand{\ip}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\der}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\dder}[2]{\frac{\partial^2 {#1}}{\partial {#2}^2}}
\usepackage{enumitem}
\newcommand{\nn}{\mathbf{n}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\uu}{\mathbf{u}}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepackage{titlesec}
\newcommand{\junk}[1]{{}}
\usepackage{sectsty}
\usepackage{xcolor}

\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
	\edef\arraystretch{#1}%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{*\c@MaxMatrixCols c}}
\makeatother

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
	\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{#1}}
\makeatother

\definecolor{darkblue}{rgb}{0,0,0.4}
\usepackage[colorlinks = true,
linkcolor = darkblue,
urlcolor  = darkblue,
citecolor = darkblue,
anchorcolor = darkblue]{hyperref}
% set two lengths for the includegraphics commands used to import the plots:
\newlength{\fwtwo} \setlength{\fwtwo}{0.45\textwidth}
% end of personal macros

\begin{document}
\DeclareGraphicsExtensions{.jpg}

\begin{center}
\textsc{\Large Statistical Pattern Recognition} \\[2pt]
	\textsc{\large Assignment 2}\\
	\vspace{0.5cm}
  Ali Gholami \\[6pt]
  Department of Computer Engineering \& Information Technology\\
  Amirkabir University of Technology  \\[6pt]
  \def\UrlFont{\em}
  \url{http://ceit.aut.ac.ir/~aligholamee}\\
    \href{mailto:aligholamee@aut.ac.ir}{\textit{aligholamee@aut.ac.ir}}
\end{center}

\begin{abstract}
In this assignment, we'll be focusing on the \textit{Bayes Classifier}. We'll work with \textit{Bayesian Discriminators} and \textit{Bayes Error}. The \textit{Bhattacharyya} error bound is also analyzed as an upper bound for the \textit{Bayes Classifier} error. The detailed computations of \textit{Bayesian Discriminators} are also given in an exact definition. Finally, we'll be going through a more practical example of a linear discriminator by classifying the flowers in the \textit{Iris} dataset.
\end{abstract}

\subparagraph{Keywords.} \textit{Linear Discriminator, Quadratic Discriminator, Bayes Classification, Bayes Error, Optimal Classification, Bhattacharyya Distance, Bhattacharyya Upper Bound, Iris Dataset, Iris Classification.}

\section{Quadratic \& Linear Discriminant Analysis}
We consider a classification problem in dimension $d=2$, with $k=3$ classes where:\\
$$
p(x\ |\ w_{i}) \sim N(\mu_{i}, \Sigma_{i}),\ \  i = 1, 2, 3
$$
and
$$
	\mu_1 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	0\\
	2\\
	\end{bmatrix},\ 
	\mu_2	 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	3\\
	1\\
	\end{bmatrix},\ 	
	\mu_3	 = 	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	1\\
	0\\
	\end{bmatrix},
	\Sigma_{i}	 = \Sigma	\renewcommand\arraystretch{1}
	\setlength\arraycolsep{6pt}
	\begin{bmatrix}
	1 & 0\\
	0 & \frac{1}{3}\\
	\end{bmatrix},
$$
\begin{enumerate}[label=(\alph*)]
	\item Calculate the discriminant function $g_{i}(x)$ for each class.
	
	\item Express your discriminant functions in the form of linear discriminant functions.
	
	\item Determine and plot the decision boundaries.
\end{enumerate}
\subsection*{Solution}
(a) The general form of a Bayesian discriminator is given below.
\begin{equation}\label{eq:1.1}
g_{i}(\underaccent{\bar}{x}) = 	-\frac{1}{2}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T}\Sigma_{i}^{-1}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i}) - \frac{1}{2}\log|\Sigma_{i}| + \log P(\omega_{i})
\end{equation}
In the problem case, the classes have the same covariance matrix, but the features have different variances.
Since the $\Sigma_{i}$ is diagonal, we'll have
$$
g_{i}(\underaccent{\bar}{x}) = 	-\frac{1}{2}(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T}
	\begin{bmatrix}
\sigma_1^{-2} & 0 & 0 & \dots & 0\\
0 & \sigma_2^{-2} & 0 & \dots & 0\\
0 & 0 & \sigma_3^{-2} & \dots & 0\\ 
\vdots & \vdots & \vdots & \ddots & 0\\
0 & 0 & 0 & 0 & \sigma_N^{-2}\\
\end{bmatrix}
(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i}) - \frac{1}{2}\log \begin{vmatrix}
\sigma_1^{-2} & 0 & 0 & \dots & 0\\
0 & \sigma_2^{-2} & 0 & \dots & 0\\
0 & 0 & \sigma_3^{-2} & \dots & 0\\ 
\vdots & \vdots & \vdots & \ddots & 0\\
0 & 0 & 0 & 0 & \sigma_N^{-2}\\
\end{vmatrix} + \log(P(\omega_{i}))
$$
Since we have the following criteria:
$$
	(\underaccent{\bar}{x} - \underaccent{\bar}{\mu}_{i})^{T} = \begin{bmatrix}
	x[1] - \mu_{i}[1]\\
	x[2] - \mu_{i}[2]\\
	x[3] - \mu_{i}[3]\\
	x[4] - \mu_{i}[4]\\
	\vdots\\
	x[N] - \mu_{i}[N]\\
	\end{bmatrix}
$$
where $\mu_{iN}$ denotes the \textit{N}'th feature of class \textit{i}. Removing the constant term for different classes, which is $ x[k]^2$, we'll have the following results after the matrix multiplication and determinant computation:
\begin{equation}
	g_{i}(\underaccent{\bar}{x}) = -\frac{1}{2}\sum_{k=1}^{N}\frac{2x[k]\mu_{i}[k] + \mu_{i}[k]^2}{\sigma_k^2} - \frac{1}{2}\log\prod_{k=1}^{N}\sigma_k^2 + \log(P(\omega_i))
\end{equation}

One can simply find each discriminator, $g_{i}(\underaccent{\bar}{x})$, by replacing the given information in the problem description in the formula given above. Thus we'll have the following results for the section (a).
$$
	g_{1}(\underaccent{\bar}{x}) = -\frac{1}{2}(\frac{2x[1]*0 + 2}{1} + \frac{2x[2]*2 + 4}{\frac{1}{9}}) - \frac{1}{2}\log(1*\frac{1}{9}) + ?
$$
$$
g_{2}(\underaccent{\bar}{x}) = -\frac{1}{2}(\frac{2x[1]*3 + 3}{1} + \frac{2x[2]*1 + 1}{\frac{1}{9}}) - \frac{1}{2}\log(1*\frac{1}{9}) + ?
$$
$$
g_{3}(\underaccent{\bar}{x}) = -\frac{1}{2}(\frac{2x[1]*1 + 1}{1} + \frac{2x[2]*0 + 0}{\frac{1}{9}}) - \frac{1}{2}\log(1*\frac{1}{9}) + ?
$$
The simplified results are
$$
	g_{1}(\underaccent{\bar}{x}) = -18x[2] - \frac{1}{2}\log\frac{1}{9} - 19
$$
$$
	g_{2}(\underaccent{\bar}{x}) = -3x[1] + 9x[2] - \frac{1}{2}\log\frac{1}{9} - 6
$$
$$
	g_{3}(\underaccent{\bar}{x}) = -x[1] - \frac{1}{2}\log\frac{1}{9} - \frac{1}{2}
$$
\noindent\rule{\textwidth}{.5pt}
(b) The final results given above where in the format of a linear discriminant already. In order to lighten everything up, just assume the linear discriminant function as:
$$
	g_{i}(\underaccent{\bar}{x}) = W_{2}x[2] + W_{1}x[1] + W_0
$$
where the value of $W_i$ is different for each of the discriminators.
$$
	g_{1}(\underaccent{\bar}{x}) \ \ \ W_2 = -18 \ \ W_1 = 0 \ \ W_0 =  - \frac{1}{2}\log\frac{1}{9} - 19
$$
$$
g_{2}(\underaccent{\bar}{x}) \ \ \ W_2 = 9 \ \ W_1 = -3 \ \ W_0 =  - \frac{1}{2}\log\frac{1}{9} - 6
$$
$$
g_{1}(\underaccent{\bar}{x}) \ \ \ W_2 = 0 \ \ W_1 = -1 \ \ W_0 =  - \frac{1}{2}\log\frac{1}{9} - \frac{1}{2}
$$
Each of the $g_{i}(\underaccent{\bar}{x})$ represent a discriminator plane in the \textit{3D} space.

\noindent\rule{\textwidth}{.5pt}
(c) 
\end{document}
